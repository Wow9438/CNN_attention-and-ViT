{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as Fun\nfrom tqdm import tqdm\n\ndevic = torch.device(\"cuda\")","metadata":{"id":"sPnMu9GpY41Q","execution":{"iopub.status.busy":"2024-03-30T11:03:18.693982Z","iopub.execute_input":"2024-03-30T11:03:18.694259Z","iopub.status.idle":"2024-03-30T11:03:23.162347Z","shell.execute_reply.started":"2024-03-30T11:03:18.694233Z","shell.execute_reply":"2024-03-30T11:03:23.161376Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:00:30.394973Z","iopub.execute_input":"2024-03-30T11:00:30.395715Z","iopub.status.idle":"2024-03-30T11:00:30.403568Z","shell.execute_reply.started":"2024-03-30T11:00:30.395677Z","shell.execute_reply":"2024-03-30T11:00:30.402431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Question 1","metadata":{}},{"cell_type":"markdown","source":"## Self Attention Module","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------first question\nclass attention:\n    def __init__(self,c,device):\n        self.device = torch.device(device)\n        self.Wq = torch.randn(c,c//8,device=device)\n        self.Wk = torch.randn(c,c//8,device=device)\n        self.Wv = torch.randn(c,c,device=device)\n        self.Wq.requires_grad_()\n        self.Wk.requires_grad_()\n        self.Wv.requires_grad_()\n    def forward(self,x,device):\n        self.device = torch.device(device)\n#         x1 = (x.view(x.size()[0],x.size()[2]*x.size()[1])).t().to(self.device)\n#         print(x.size())\n        Q = torch.matmul(x,self.Wq).to(self.device)\n        K = torch.matmul(x,self.Wk).to(self.device)\n#         sqd = torch.tensor((x.size()[1]+0.001)**0.5) /sqd.to(self.device)\n        E = torch.matmul(Q,K.t())\n        A = torch.softmax(E, dim=1).to(self.device)\n        V = torch.matmul(x,self.Wv).to(self.device)\n        Y = torch.matmul(A,V).to(self.device)\n        return Y","metadata":{"id":"udss4BWcklU-","execution":{"iopub.status.busy":"2024-03-30T12:05:10.686304Z","iopub.execute_input":"2024-03-30T12:05:10.686809Z","iopub.status.idle":"2024-03-30T12:05:10.695691Z","shell.execute_reply.started":"2024-03-30T12:05:10.686766Z","shell.execute_reply":"2024-03-30T12:05:10.694765Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Self attention on CNN","metadata":{}},{"cell_type":"code","source":"class CNN_Transformer_(nn.Module):\n    def __init__(self,device):\n        \n        super(CNN_Transformer_, self).__init__()\n        \n        self.device = torch.device(device)\n        self.ReLU = nn.ReLU()\n        \n        self.conv1 = nn.Conv2d(3, 32, 5,device=device)\n        self.atten1 = attention(32,self.device)\n        self.norm1 = nn.LayerNorm(normalized_shape = 32, device = device)\n        \n        self.conv2 = nn.Conv2d(32, 64, 5,device=device)\n        self.atten2 = attention(64,self.device)\n        self.norm2 = nn.LayerNorm(normalized_shape = 64, device = device)\n        self.pool = nn.MaxPool2d(2,2)\n        self.conv3 = nn.Conv2d(64, 128, 5,device=device)\n        self.atten3 = attention(128,self.device)\n        self.norm3 = nn.LayerNorm(normalized_shape = 128, device = device)\n        \n        self.conv4 = nn.Conv2d(128, 256, 5,device=device)\n        #GAP here\n        self.mlp1 = nn.Linear(256,125,device=device)\n        self.mlp2 = nn.Linear(125,63,device=device)\n        self.mlp3 = nn.Linear(63,10,device=device)\n        \n    def forward(self,x:torch.Tensor,device:str):\n    \n        self.device = torch.device(device)\n        \n        out1 = self.conv1(x).to(self.device)\n        o = out1.view(  out1.size()[0]  ,  out1.size()[1]*out1.size()[2]  ).t()\n        out2 = self.atten1.forward(o,self.device).to(self.device)\n        out2 = torch.reshape(out2.t(),(out1.size()[1],out1.size()[2],out1.size()[0]))\n        out2 = self.norm1(out2)\n        out2 = torch.reshape(out2,(out1.size()[0],out1.size()[1],out1.size()[2]))\n        out2 = torch.add(out1,out2)\n        \n        \n        out3 = self.conv2(out2).to(self.device)\n        o = out3.view(out3.size()[0],out3.size()[1]*out3.size()[2]).t()\n        out4 = self.atten2.forward(o,self.device).to(self.device)\n        out4 = torch.reshape(out4.t(),(out3.size()[1],out3.size()[2],out3.size()[0]))\n        out4 = self.norm2(out4)\n        out4 = torch.reshape(out4,(out3.size()[0],out3.size()[1],out3.size()[2]))\n        out4 = torch.add(out3,out4)#residual network\n        \n        \n        out4 = self.pool(out4)\n        \n        out5 = self.conv3(out4).to(self.device)\n        o = out5.view(out5.size()[0],out5.size()[1]*out5.size()[2]).t()\n        out6 = self.atten3.forward(o,self.device).to(self.device)\n        out6 = torch.reshape(out6.t(),(out5.size()[1],out5.size()[2],out5.size()[0]))\n        out6 = self.norm3(out6)\n        out6 = torch.reshape(out6,(out5.size()[0],out5.size()[1],out5.size()[2]))\n        out6 = torch.add(out5,out6)#residual network\n        \n        out7 = self.conv4(out6).to(self.device)\n        out8 = (torch.einsum('ijk->i',out7))/(torch.tensor(out7.size()[1]*out7.size()[2])).to(self.device)\n        out9 = self.ReLU(self.mlp1(out8))\n        out10 = self.ReLU(self.mlp2(out9))\n        out11 = Fun.softmax(self.mlp3(out10),dim=0).to(self.device)\n        return out11","metadata":{"execution":{"iopub.status.busy":"2024-03-30T12:09:53.212403Z","iopub.execute_input":"2024-03-30T12:09:53.213416Z","iopub.status.idle":"2024-03-30T12:09:53.235464Z","shell.execute_reply.started":"2024-03-30T12:09:53.213373Z","shell.execute_reply":"2024-03-30T12:09:53.234246Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Importing Data","metadata":{"id":"atcf46ZtlUsT"}},{"cell_type":"code","source":"import torchvision\nfrom torchvision import transforms\n#import CIFAR10\n\ntransform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n\nTrain= torchvision.datasets .CIFAR10(root='./data',train = True,download=True,transform=transform)\nTest= torchvision.datasets .CIFAR10(root='./data',train = False,download=True,transform=transform)","metadata":{"id":"i9bvJEGxlUCK","outputId":"a22c254d-0008-4606-de85-996d67556797","execution":{"iopub.status.busy":"2024-03-30T12:09:54.320132Z","iopub.execute_input":"2024-03-30T12:09:54.320747Z","iopub.status.idle":"2024-03-30T12:09:55.994633Z","shell.execute_reply.started":"2024-03-30T12:09:54.320716Z","shell.execute_reply":"2024-03-30T12:09:55.993770Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(dataset = Train,batch_size=1,shuffle = True)\ntest_loader = torch.utils.data.DataLoader(dataset = Test, batch_size=1,shuffle = False)","metadata":{"id":"ehp6SuPKvL1-","execution":{"iopub.status.busy":"2024-03-30T12:09:55.996199Z","iopub.execute_input":"2024-03-30T12:09:55.996494Z","iopub.status.idle":"2024-03-30T12:09:56.005188Z","shell.execute_reply.started":"2024-03-30T12:09:55.996469Z","shell.execute_reply":"2024-03-30T12:09:56.004147Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"using cross entropy loss i.e;\n\n$\\Rightarrow J=-\\sum_{i=0}^{9}(p_i \\ast log(p_i))$ for each training point\n\n$\\Rightarrow J = -p_i*log(p_i)$ ,where $i$ is the index at which ground truth occurs\n","metadata":{"id":"QC42rSE0m3VD"}},{"cell_type":"code","source":"\nmodel = CNN_Transformer_('cuda')\nmodel.to(devic)","metadata":{"id":"SmoDC7EeqL4S","outputId":"f2bf2f5e-7359-4b32-b8b6-dbbcc1a56163","execution":{"iopub.status.busy":"2024-03-30T12:35:47.990459Z","iopub.execute_input":"2024-03-30T12:35:47.990870Z","iopub.status.idle":"2024-03-30T12:35:48.000541Z","shell.execute_reply.started":"2024-03-30T12:35:47.990844Z","shell.execute_reply":"2024-03-30T12:35:47.999731Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"CNN_Transformer_(\n  (ReLU): ReLU()\n  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n  (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n  (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n  (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n  (conv4): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1))\n  (mlp1): Linear(in_features=256, out_features=125, bias=True)\n  (mlp2): Linear(in_features=125, out_features=63, bias=True)\n  (mlp3): Linear(in_features=63, out_features=10, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"epochs = 3\nlr = 0.01\nbatch_size = 32\nfor i in tqdm(range(epochs)):\n    for j , (images, label) in enumerate(train_loader):\n\n        images = images[0].to(devic)\n        label = label.to(devic)\n        p = model.forward(images,'cuda')\n        lp = -torch.log(p).to(devic)\n        \n        Loss = lp[label].to(devic)\n        \n        if j%10000 == 0:\n            print(Loss)\n        \n        Loss.backward()\n        \n    \n        if((j+1)%batch_size == 0 or (j+1)==50000):\n            with torch.no_grad():\n                model.mlp3.weight.data -= lr*(model.mlp3.weight.grad)/batch_size\n                model.mlp3.bias.data -= lr*(model.mlp3.bias.grad)/batch_size\n\n                model.mlp2.weight.data -= lr*(model.mlp2.weight.grad)/batch_size\n                model.mlp2.bias.data -= lr*(model.mlp2.bias.grad)/batch_size\n\n                model.mlp1.weight.data -= lr*(model.mlp1.weight.grad)/batch_size\n                model.mlp1.bias.data -= lr*(model.mlp1.bias.grad)/batch_size\n\n                model.conv4.weight.data -= lr*(model.conv4.weight.grad)/batch_size\n                model.conv4.bias.data -= lr*(model.conv4.bias.grad)/batch_size\n\n                model.atten3.Wq.data -= lr*(model.atten3.Wq.grad)/batch_size\n                model.atten3.Wk.data -= lr*(model.atten3.Wk.grad)/batch_size\n                model.atten3.Wv.data -= lr*(model.atten3.Wv.grad)/batch_size\n\n                model.conv3.weight.data -= lr*(model.conv3.weight.grad)/batch_size\n                model.conv3.bias.data -= lr*(model.conv3.bias.grad)/batch_size\n\n                model.atten2.Wq.data -= lr*(model.atten2.Wq.grad)/batch_size\n                model.atten2.Wk.data -= lr*(model.atten2.Wk.grad)/batch_size\n                model.atten2.Wv.data -= lr*(model.atten2.Wv.grad)/batch_size\n\n                model.conv2.weight.data -= lr*(model.conv2.weight.grad)/batch_size\n                model.conv2.bias.data -= lr*(model.conv2.bias.grad)/batch_size\n\n                model.atten1.Wq.data -= lr*(model.atten1.Wq.grad)/batch_size\n                model.atten1.Wk.data -= lr*(model.atten1.Wk.grad)/batch_size\n                model.atten1.Wv.data -= lr*(model.atten1.Wv.grad/batch_size)\n\n                model.conv1.weight.data -= lr*(model.conv1.weight.grad)/batch_size\n                model.conv1.bias.data -= lr*(model.conv1.bias.grad)/batch_size\n\n\n\n            model.mlp3.weight.grad.zero_()\n            model.mlp3.bias.grad.zero_()\n\n            model.mlp2.weight.grad.zero_()\n            model.mlp2.bias.grad.zero_()\n\n            model.mlp1.weight.grad.zero_()\n            model.mlp1.bias.grad.zero_()\n\n            model.conv4.weight.grad.zero_()\n            model.conv4.bias.grad.zero_()\n\n            model.atten3.Wq.grad.zero_()\n            model.atten3.Wk.grad.zero_()\n            model.atten3.Wv.grad.zero_()\n\n            model.conv3.weight.grad.zero_()\n            model.conv3.bias.grad.zero_()\n\n            model.atten2.Wq.grad.zero_()\n            model.atten2.Wk.grad.zero_()\n            model.atten2.Wv.grad.zero_()\n\n\n            model.conv2.weight.grad.zero_()\n            model.conv2.bias.grad.zero_()\n\n            model.atten1.Wq.grad.zero_()\n            model.atten1.Wk.grad.zero_()\n            model.atten1.Wv.grad.zero_()\n\n            model.conv1.weight.grad.zero_()\n            model.conv1.bias.grad.zero_()\n","metadata":{"id":"dre5VREBvyDP","outputId":"ed14fd81-80c9-47f8-9e6c-78c67dd2f0e5","execution":{"iopub.status.busy":"2024-03-30T12:35:52.843151Z","iopub.execute_input":"2024-03-30T12:35:52.844039Z","iopub.status.idle":"2024-03-30T12:48:57.428181Z","shell.execute_reply.started":"2024-03-30T12:35:52.844005Z","shell.execute_reply":"2024-03-30T12:48:57.427274Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"  0%|          | 0/3 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"tensor([2.3518], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.7155], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([0.4370], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.0364], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.3115], device='cuda:0', grad_fn=<IndexBackward0>)\n","output_type":"stream"},{"name":"stderr","text":" 33%|███▎      | 1/3 [04:22<08:45, 262.70s/it]","output_type":"stream"},{"name":"stdout","text":"tensor([1.8351], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([0.7792], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.6258], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.6567], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([0.6100], device='cuda:0', grad_fn=<IndexBackward0>)\n","output_type":"stream"},{"name":"stderr","text":" 67%|██████▋   | 2/3 [08:43<04:21, 261.59s/it]","output_type":"stream"},{"name":"stdout","text":"tensor([1.8040], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.5332], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.9919], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([5.0020], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.3589], device='cuda:0', grad_fn=<IndexBackward0>)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3/3 [13:04<00:00, 261.52s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"\ncorrect = torch.tensor(0)\nloss = torch.tensor(0.00000001)\nfor i , (images, label) in enumerate(test_loader):\n\n    images = images[0].to(devic)\n    label = label.to(devic)\n    if (i%1000==0) :\n        print(i//1000)\n    p = model.forward(images,'cuda')\n#     lp = -torch.log(p).to('cpu')\n#     print(loss.device)\n#     loss  -= torch.max(lp)\n    \n    if(int(torch.argmax(p))==label):\n        correct = correct+1\n        \n#     del out\n#     torch.cuda.empty_cache()\n    \ntest_accuracy = correct/10000","metadata":{"id":"gzp2rlLhbS9o","outputId":"a50e03bb-1205-44a3-f02a-9e10d50e7ac8","execution":{"iopub.status.busy":"2024-03-30T12:49:13.268811Z","iopub.execute_input":"2024-03-30T12:49:13.269737Z","iopub.status.idle":"2024-03-30T12:49:39.790983Z","shell.execute_reply.started":"2024-03-30T12:49:13.269701Z","shell.execute_reply":"2024-03-30T12:49:39.790130Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n","output_type":"stream"}]},{"cell_type":"code","source":"test_accuracy*100","metadata":{"execution":{"iopub.status.busy":"2024-03-30T12:49:42.980434Z","iopub.execute_input":"2024-03-30T12:49:42.981074Z","iopub.status.idle":"2024-03-30T12:49:42.988505Z","shell.execute_reply.started":"2024-03-30T12:49:42.981042Z","shell.execute_reply":"2024-03-30T12:49:42.987595Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"tensor(39.7800)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Question 2","metadata":{}},{"cell_type":"code","source":"class multi_attention:\n    def __init__(self,d:int,h:int,device:str):\n        self.device = torch.device(device)\n        self.Wq = torch.randn(h,d//h,d//h,device=device)\n        self.Wk = torch.randn(h,d//h,d//h,device=device)\n        self.Wv = torch.randn(h,d//h,d//h,device=device)\n        self.Wq.requires_grad_()\n        self.Wk.requires_grad_()\n        self.Wv.requires_grad_()\n    def forward(self,x:torch.tensor,h:int,device:str):\n        self.device = torch.device(device)\n        x1 = torch.reshape(x,(x.size()[0],h,x.size()[1]//h)).to(self.device)\n#         print(x1.size(),self.Wq.size())\n        Q = torch.einsum('ikj,kjl->ikl',x1,self.Wq).to(self.device)\n        K = torch.einsum('ikj,kjl->ikl',x1,self.Wk).to(self.device)\n        sqd = torch.tensor((x1.size()[2]+0.001)**0.5)\n        E = torch.einsum('ijk,ljk->jil',Q,K)/sqd.to(self.device)\n        A = torch.softmax(E, dim=2).to(self.device)\n        V = torch.einsum('ikj,kjl->ikl',x1,self.Wv).to(self.device)\n        Y = torch.einsum('ikj,jil->kil',A,V).to(self.device)\n        return torch.reshape(Y,(x.size()[0],x.size()[1]))","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:27:10.849192Z","iopub.execute_input":"2024-03-30T11:27:10.849878Z","iopub.status.idle":"2024-03-30T11:27:10.860181Z","shell.execute_reply.started":"2024-03-30T11:27:10.849846Z","shell.execute_reply":"2024-03-30T11:27:10.859201Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class ViT(nn.Module):\n    def __init__(self,p:int,h1:int,h2:int,device:str,size=3*32*32):\n        super(ViT,self).__init__()\n        self.ReLU = nn.ReLU\n        self.device = torch.device(device)\n        self.xclass = torch.randn(1,3*p*p,device=device)\n        self.xclass.requires_grad_()\n        self.pos = torch.randn(1+(size//(3*p*p)),3*p*p,device=device)\n        self.pos.requires_grad_()\n        self.atten1 = multi_attention(3*p*p,h1,device=device)\n        self.norm1 = nn.LayerNorm(normalized_shape=3,device = device)\n        \n        \n        self.atten2 = multi_attention(3*p*p,h2,device=device)\n        self.norm2 = nn.LayerNorm(normalized_shape=3,device = device)\n        self.mlp = nn.Linear(3*p*p,10,device=device)\n        \n    def forward(self,x:torch.Tensor,h1:int,h2:int,p:int,device:str):\n        self.device = torch.device(device)\n        x1 = torch.reshape(x,(3*p*p,(x.size()[0]*x.size()[1]*x.size()[2])//(3*p*p))).t().to(self.device)\n        \n        xemb = torch.cat((self.xclass,x1),dim=0)\n        x2 = torch.add(xemb,self.pos)\n        \n        out1 = self.atten1.forward(x2,h1,device=device).to(self.device)\n        out1 = torch.reshape(out1,(p*p+x.size()[1]*x.size()[2],x.size()[0]))\n        out1 = torch.reshape(self.norm1(out1),x2.size())\n        out1 = torch.add(x2,out1).to(self.device)\n#         outl1 = self.ReLU(self.l1(out1.view(-1)))\n#         out1 = torch.reshape(outl1,out1.size())\n        \n        out2 = self.atten2.forward(out1,h2,device=device).to(self.device)\n        out2 = torch.reshape(out2,(p*p+x.size()[1]*x.size()[2],x.size()[0]))\n        out2 = torch.reshape(self.norm2(out2),x2.size())\n        out2 = torch.add(out2,out1).to(self.device)\n#         outl2 = self.ReLU(self.l2(out2.view(-1)))\n#         out2 = torch.reshape(outl2,out2.size())\n        \n        out3 = self.mlp(out2[0]).to(self.device)\n        out3 = Fun.softmax(out3,dim=0).to(self.device)\n        return out3","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:27:12.431966Z","iopub.execute_input":"2024-03-30T11:27:12.432670Z","iopub.status.idle":"2024-03-30T11:27:12.447588Z","shell.execute_reply.started":"2024-03-30T11:27:12.432638Z","shell.execute_reply":"2024-03-30T11:27:12.446376Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"ViT_model = ViT(4,3,2,'cuda')\nViT_model.to(devic)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:27:14.025650Z","iopub.execute_input":"2024-03-30T11:27:14.026018Z","iopub.status.idle":"2024-03-30T11:27:14.034161Z","shell.execute_reply.started":"2024-03-30T11:27:14.025991Z","shell.execute_reply":"2024-03-30T11:27:14.033249Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"ViT(\n  (norm1): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n  (norm2): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n  (mlp): Linear(in_features=48, out_features=10, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"epochs = 10\nlr = 0.01\nbatch_size=32\nfor i in tqdm(range(epochs)):\n    for j , (images, label) in enumerate(train_loader):\n\n        images = images[0].to(devic)\n        label = label.to(devic)\n        \n        p = ViT_model.forward(images,3,2,4,'cuda')\n        lp = -torch.log(p).to(devic)\n\n        Loss = lp[label].to(devic)\n        \n        if j%10000==0:\n            print(Loss)\n            \n        Loss.backward()\n        \n        if((j+1)%batch_size == 0 or (j+1)==50000):\n            with torch.no_grad():\n                ViT_model.mlp.weight.data -= lr*(ViT_model.mlp.weight.grad)/batch_size\n                ViT_model.mlp.bias.data -= lr*(ViT_model.mlp.bias.grad)/batch_size\n\n                ViT_model.atten2.Wq.data -= lr*(ViT_model.atten2.Wq.grad)/batch_size\n                ViT_model.atten2.Wk.data -= lr*(ViT_model.atten2.Wk.grad)/batch_size\n                ViT_model.atten2.Wv.data -= lr*(ViT_model.atten2.Wv.grad)/batch_size\n\n                ViT_model.atten1.Wq.data -= lr*(ViT_model.atten1.Wq.grad)/batch_size\n                ViT_model.atten1.Wk.data -= lr*(ViT_model.atten1.Wk.grad)/batch_size\n                ViT_model.atten1.Wv.data -= lr*(ViT_model.atten1.Wv.grad)/batch_size\n\n                ViT_model.pos.data -= lr*(ViT_model.pos.grad)/batch_size\n                ViT_model.xclass.data -= lr*(ViT_model.xclass.grad)/batch_size\n\n\n            ViT_model.mlp.weight.grad.zero_()\n            ViT_model.mlp.bias.grad.zero_()\n\n            ViT_model.atten2.Wq.grad.zero_()\n            ViT_model.atten2.Wk.grad.zero_()\n            ViT_model.atten2.Wv.grad.zero_()\n\n            ViT_model.atten1.Wq.grad.zero_()\n            ViT_model.atten1.Wk.grad.zero_()\n            ViT_model.atten1.Wv.grad.zero_()\n            ViT_model.pos.grad.zero_()\n            ViT_model.xclass.grad.zero_()","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:27:16.097194Z","iopub.execute_input":"2024-03-30T11:27:16.097581Z","iopub.status.idle":"2024-03-30T12:01:52.756688Z","shell.execute_reply.started":"2024-03-30T11:27:16.097540Z","shell.execute_reply":"2024-03-30T12:01:52.755775Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"  0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"tensor([2.6360], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.8599], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.2932], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.0129], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.1514], device='cuda:0', grad_fn=<IndexBackward0>)\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 1/10 [03:26<31:01, 206.78s/it]","output_type":"stream"},{"name":"stdout","text":"tensor([1.7865], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.8314], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.3291], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.0232], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.5417], device='cuda:0', grad_fn=<IndexBackward0>)\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 2/10 [06:54<27:39, 207.43s/it]","output_type":"stream"},{"name":"stdout","text":"tensor([2.3163], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.4737], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([3.5836], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.5622], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.4242], device='cuda:0', grad_fn=<IndexBackward0>)\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 3/10 [10:21<24:10, 207.26s/it]","output_type":"stream"},{"name":"stdout","text":"tensor([3.2950], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.0237], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.3869], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.8374], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.8520], device='cuda:0', grad_fn=<IndexBackward0>)\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 4/10 [13:48<20:43, 207.22s/it]","output_type":"stream"},{"name":"stdout","text":"tensor([2.2697], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.7554], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.3616], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.5453], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.5501], device='cuda:0', grad_fn=<IndexBackward0>)\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 5/10 [17:15<17:14, 206.86s/it]","output_type":"stream"},{"name":"stdout","text":"tensor([1.8131], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.0909], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.4412], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.7474], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.2020], device='cuda:0', grad_fn=<IndexBackward0>)\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 6/10 [20:43<13:49, 207.34s/it]","output_type":"stream"},{"name":"stdout","text":"tensor([3.0648], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.7399], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.6138], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.6857], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.6973], device='cuda:0', grad_fn=<IndexBackward0>)\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 7/10 [24:10<10:21, 207.19s/it]","output_type":"stream"},{"name":"stdout","text":"tensor([2.5756], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.9276], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([2.0033], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.3354], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.4647], device='cuda:0', grad_fn=<IndexBackward0>)\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 8/10 [27:36<06:53, 206.96s/it]","output_type":"stream"},{"name":"stdout","text":"tensor([2.2786], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.6687], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.7540], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.9209], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.4763], device='cuda:0', grad_fn=<IndexBackward0>)\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 9/10 [31:08<03:28, 208.33s/it]","output_type":"stream"},{"name":"stdout","text":"tensor([1.5334], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.1407], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.5215], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([3.7283], device='cuda:0', grad_fn=<IndexBackward0>)\ntensor([1.4710], device='cuda:0', grad_fn=<IndexBackward0>)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [34:36<00:00, 207.66s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"\ncorrect = torch.tensor(0)\nloss = torch.tensor(0.00000001)\nfor i , (images, label) in enumerate(test_loader):\n\n    images = images[0].to(devic)\n    label = label.to(devic)\n    if (i%1000==0) :\n        print(i//1000)\n    p = ViT_model.forward(images,3,2,4,'cuda')\n    \n    if(int(torch.argmax(p))==label):\n        correct = correct+1\n        \n#     del out\n#     torch.cuda.empty_cache()\n    \ntest_accuracy_2 = correct/10000","metadata":{"execution":{"iopub.status.busy":"2024-03-30T12:31:07.381668Z","iopub.execute_input":"2024-03-30T12:31:07.382173Z","iopub.status.idle":"2024-03-30T12:31:28.172809Z","shell.execute_reply.started":"2024-03-30T12:31:07.382137Z","shell.execute_reply":"2024-03-30T12:31:28.171638Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n","output_type":"stream"}]},{"cell_type":"code","source":"test_accuracy_2*100","metadata":{"execution":{"iopub.status.busy":"2024-03-30T12:31:31.056128Z","iopub.execute_input":"2024-03-30T12:31:31.056510Z","iopub.status.idle":"2024-03-30T12:31:31.064233Z","shell.execute_reply.started":"2024-03-30T12:31:31.056480Z","shell.execute_reply":"2024-03-30T12:31:31.063224Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"tensor(30.4400)"},"metadata":{}}]},{"cell_type":"markdown","source":"\nWe can see that the accuracy of the Vision Transformer like model is lesser than that of the model involving convolutions and self attention.\n\nIt is known that encoder only models are data hungry and require a huge amount of data for them to overpower the convolutional models.","metadata":{}}]}